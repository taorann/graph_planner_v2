experiment:
  name: planner-debug
  seed: 42
  notes: "Single GPU debug launch"
paths:
  dataset_train: datasets/r2e_gym/train.jsonl
  dataset_val: datasets/r2e_gym/val.jsonl
  planner_model: models/Qwen3-14B
  planner_tokenizer: models/Qwen3-14B
  cgm_model: models/CodeFuse-CGM
  cgm_tokenizer: models/CodeFuse-CGM
backends:
  planner_backend: local
  cgm_backend: local
  dtype: float16
  device_map_planner: [0]
  device_map_cgm: [0]
  max_gpu_memory: null
planner_sampling:
  temperature: 0.2
  top_p: 0.95
  top_k: 50
  max_input_tokens: 4096
  max_new_tokens: 512
  repetition_penalty: 1.0
  do_sample: false
  stop: []
  stop_ids: []
  stop_on_invalid_json: true
cgm_generation:
  temperature: 0.2
  top_p: 0.9
  top_k: 40
  max_new_tokens: 768
  num_return_sequences: 1
  do_sample: false
training:
  total_epochs: 1
  train_batch_size: 1
  grad_accum_steps: 1
  precision: fp16
  lr: 5.0e-05
  weight_decay: 0.0
  warmup_steps: 0
  gradient_checkpointing: false
  clip_grad_norm: 1.0
  kl_coef: 0.1
  entropy_coef: 0.01
  value_coef: 1.0
  clip_coef: 0.2
  target_kl: null
  total_steps: null
  resume_from: null
env:
  max_steps: 6
  reward_scale: 1.0
  failure_penalty: -1.0
  step_penalty: -0.05
  timeout_penalty: -0.5
  repo_op_limit: 32
  disable_cgm_synthesis: false
  apply_patches: false
parallel:
  tensor_parallel_planner: 1
  tensor_parallel_cgm: 1
  replicas: 1
  parallel_agents: 1
  rollout_workers: 1
  workflow_parallel: 1
resources:
  num_gpus: 1
  num_nodes: 1
  ray_num_gpus: 1
  ray_num_cpus: 4
  ray_memory: null
  ray_object_store_memory: null
logging:
  wandb:
    enabled: false
    offline: true
    project: graph-planner
    entity: null
    run_name: planner-debug
    watch:
      enabled: false
      log: gradients
      log_freq: 200
  log_backend: wandb
  output_dir: outputs
  save_interval: 200
  eval_interval: 200
telemetry:
  log_gpu: false
  log_ray: false
  log_patch_stats: true
  log_planner_parse_errors: true
  log_cgm_errors: true
verl_overrides:
  trainer:
    n_gpus_per_node: 1
    gradient_accumulation_steps: 1
  actor_rollout_ref:
    model:
      enable_gradient_checkpointing: false
    actor:
      strategy: fsdp
      use_kl_loss: false
      grad_clip: 1.0
      entropy_coeff: 0.01
      clip_ratio: 0.2
      clip_ratio_low: 0.2
      clip_ratio_high: 0.2
      policy_loss:
        ppo_kl_coef: 0.1
      optim:
        lr: 5.0e-05
        weight_decay: 0.0
        lr_warmup_steps: 0
    rollout:
      strategy: fsdp
      dtype: float16
    ref:
      strategy: fsdp
  critic:
    grad_clip: 1.0
    optim:
      lr: 5.0e-05
      weight_decay: 0.0
      lr_warmup_steps: 0
    model:
      enable_gradient_checkpointing: false
  algorithm:
    use_kl_in_reward: false
    kl_ctrl:
      kl_coef: 0.1
