<<<<<<< HEAD
experiment:
  name: planner-cgm-test4g
  seed: 1234
  notes: "Planner+CGM smoke test on 4 GPUs"
paths:
  dataset_train: datasets/r2e_gym/train.jsonl
  dataset_val: datasets/r2e_gym/val.jsonl
  planner_model: models/Qwen3-14B
  planner_tokenizer: models/Qwen3-14B
  cgm_model: models/CodeFuse-CGM
  cgm_tokenizer: models/CodeFuse-CGM
backends:
  planner_backend: local
  cgm_backend: local
  dtype: bfloat16
  device_map_planner: [0,1]
  device_map_cgm: [2,3]
  max_gpu_memory: null
planner_sampling:
  temperature: 0.2
  top_p: 0.95
  top_k: 50
  max_input_tokens: 4096
  max_new_tokens: 512
  repetition_penalty: 1.0
  do_sample: false
  stop: []
  stop_ids: []
  stop_on_invalid_json: true
cgm_generation:
  temperature: 0.2
  top_p: 0.9
  top_k: 40
  max_new_tokens: 768
  num_return_sequences: 1
  do_sample: false
training:
  total_epochs: 1
  train_batch_size: 2
  grad_accum_steps: &grad_accum 4
  precision: bf16
  lr: &lr 3.0e-05
  weight_decay: &weight_decay 0.01
  warmup_steps: &warmup 200
  gradient_checkpointing: &grad_ckpt true
  clip_grad_norm: &grad_clip 1.0
  kl_coef: &kl 0.1
  entropy_coef: &entropy 0.01
  value_coef: 1.0
  clip_coef: &clip 0.2
  target_kl: &target_kl 0.15
  total_steps: null
  resume_from: null
env:
  max_steps: 6
  reward_scale: 1.0
  failure_penalty: -1.0
  step_penalty: -0.05
  timeout_penalty: -0.5
  repo_op_limit: 48
  disable_cgm_synthesis: false
  apply_patches: true
parallel:
  tensor_parallel_planner: 2
  tensor_parallel_cgm: 2
  replicas: 1
  parallel_agents: 2
  rollout_workers: 2
  workflow_parallel: 2
resources:
  num_gpus: 4
  num_nodes: 1
  ray_num_gpus: 4
  ray_num_cpus: 16
  ray_memory: null
  ray_object_store_memory: null
logging:
  wandb:
    enabled: true
    offline: false
    project: graph-planner
    entity: null
    run_name: planner-cgm-test4g
    watch:
      enabled: true
      log: gradients
      log_freq: 200
  log_backend: wandb
  output_dir: outputs
  save_interval: 500
  eval_interval: 500
telemetry:
  log_gpu: true
  log_ray: true
  log_patch_stats: true
  log_planner_parse_errors: true
  log_cgm_errors: true
verl_overrides:
  trainer:
    n_gpus_per_node: 4
    gradient_accumulation_steps: *grad_accum
  actor_rollout_ref:
    model:
      enable_gradient_checkpointing: *grad_ckpt
    actor:
      grad_clip: *grad_clip
      entropy_coeff: *entropy
      clip_ratio: *clip
      clip_ratio_low: *clip
      clip_ratio_high: *clip
      policy_loss:
        ppo_kl_coef: *kl
      optim:
        lr: *lr
        weight_decay: *weight_decay
        lr_warmup_steps: *warmup
    rollout:
      dtype: bfloat16
  critic:
    grad_clip: *grad_clip
    optim:
      lr: *lr
      weight_decay: *weight_decay
      lr_warmup_steps: *warmup
    model:
      enable_gradient_checkpointing: *grad_ckpt
  algorithm:
    kl_ctrl:
      kl_coef: *kl
      target_kl: *target_kl
=======
# configs/experiments/test4g.yaml

common:
  # 统一的环境与 Ray
  env_class: your_pkg.envs.repo_repair:RepoRepairEnv
  agent_class: your_pkg.agents.planner:PlannerAgent     # planner 默认；cgm 会单独覆盖
  ray_init:
    address: null
    num_cpus: 16
    num_gpus: 4
    _system_config:
      object_spilling_config: null

  # 训练存档
  trainer:
    save_freq: 1000
    test_freq: 1000

  # 数据（可被各角色覆盖）
  data:
    train_files: /abs/path/r2e_gym_train.parquet
    val_files:   /abs/path/r2e_gym_val.parquet
    max_prompt_length: 4096
    max_response_length: 768

# ---- planner 角色（14B）----
planner:
  resource:
    num_gpus: 2
  agent_class: your_pkg.agents.planner:PlannerAgent
  env_class: your_pkg.envs.repo_repair:RepoRepairEnv

  data:
    train_files: /abs/path/r2e_gym_train.parquet
    val_files:   /abs/path/r2e_gym_val.parquet
    max_prompt_length: 8192
    max_response_length: 1024

  actor_rollout_ref:
    model:
      path: Qwen/Qwen2.5-14B-Instruct
      trust_remote_code: true
      enable_gradient_checkpointing: true
    actor:
      ppo_micro_batch_size_per_gpu: 1
      ppo_mini_batch_size: 64
      optim:
        lr: 5.0e-6
        total_training_steps: 10000
    rollout:
      name: hf          # 4卡并行双训时建议先用 HF，避免起多个 vLLM 进程
      temperature: 1.0
      top_p: 1.0
      n: 1

  agent:
    n_parallel_agents: 16
    max_steps: 10
    use_stepwise_advantage: true

# ---- CGM 角色（73B，建议 LoRA）----
cgm:
  resource:
    num_gpus: 2
  agent_class: your_pkg.agents.cgm:CGMAgent   # 需要你实现的 CGM agent 类
  env_class: your_pkg.envs.repo_repair:RepoRepairEnv

  data:
    train_files: /abs/path/r2e_gym_train.parquet
    val_files:   /abs/path/r2e_gym_val.parquet
    max_prompt_length: 4096
    max_response_length: 512

  actor_rollout_ref:
    model:
      path: Qwen/Qwen2.5-72B-Instruct
      trust_remote_code: true
      enable_gradient_checkpointing: true
      lora_rank: 16            # 建议 LoRA 以适配 2×A800
      lora_alpha: 32
      lora_dropout: 0.05
      target_modules: all-linear
    actor:
      ppo_micro_batch_size_per_gpu: 1
      ppo_mini_batch_size: 32
      optim:
        lr: 2.0e-6
        total_training_steps: 8000
    rollout:
      name: hf
      temperature: 1.0
      top_p: 1.0
      n: 1

  agent:
    n_parallel_agents: 8
    max_steps: 8

# （可选）再放一层覆盖
# planner:
#   role_overrides: {}
# cgm:
#   role_overrides: {}
>>>>>>> 3245369 (	new file:   configs/experiments/test4g.yaml)
