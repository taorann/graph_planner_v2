experiment:
  name: planner-8g
  seed: 1234
  notes: "Planner GRPO with 8 GPUs"
paths:
  dataset_train: datasets/r2e_gym/train.jsonl
  dataset_val: datasets/r2e_gym/val.jsonl
  planner_model: models/Qwen3-14B
  planner_tokenizer: models/Qwen3-14B
  cgm_model: models/CodeFuse-CGM
  cgm_tokenizer: models/CodeFuse-CGM
backends:
  planner_backend: local
  cgm_backend: local
  dtype: bfloat16
  device_map_planner: [0,1,2,3]
  device_map_cgm: [4,5,6,7]
  max_gpu_memory: null
planner_sampling:
  temperature: 0.2
  top_p: 0.95
  top_k: 50
  max_input_tokens: 4096
  max_new_tokens: 512
  repetition_penalty: 1.0
  do_sample: false
  stop: []
  stop_ids: []
  stop_on_invalid_json: true
cgm_generation:
  temperature: 0.2
  top_p: 0.9
  top_k: 40
  max_new_tokens: 768
  num_return_sequences: 1
  do_sample: false
training:
  total_epochs: 1
  train_batch_size: 4
  grad_accum_steps: 8
  precision: bf16
  lr: 3.0e-05
  weight_decay: 0.01
  warmup_steps: 200
  gradient_checkpointing: true
  clip_grad_norm: 1.0
  kl_coef: 0.1
  entropy_coef: 0.01
  value_coef: 1.0
  clip_coef: 0.2
  target_kl: 0.15
  total_steps: null
  resume_from: null
env:
  max_steps: 6
  reward_scale: 1.0
  failure_penalty: -1.0
  step_penalty: -0.05
  timeout_penalty: -0.5
  repo_op_limit: 64
  disable_cgm_synthesis: false
  apply_patches: true
parallel:
  tensor_parallel_planner: 4
  tensor_parallel_cgm: 4
  replicas: 1
  parallel_agents: 4
  rollout_workers: 4
  workflow_parallel: 4
resources:
  num_gpus: 8
  num_nodes: 1
  ray_num_gpus: 8
  ray_num_cpus: 32
  ray_memory: null
  ray_object_store_memory: null
logging:
  wandb:
    enabled: true
    offline: false
    project: graph-planner
    entity: null
    run_name: planner-8g
    watch:
      enabled: true
      log: gradients
      log_freq: 200
  log_backend: wandb
  output_dir: outputs
  save_interval: 500
  eval_interval: 500
telemetry:
  log_gpu: true
  log_ray: true
  log_patch_stats: true
  log_planner_parse_errors: true
  log_cgm_errors: true
verl_overrides:
  trainer:
    n_gpus_per_node: 8
    gradient_accumulation_steps: 8
  actor_rollout_ref:
    model:
      enable_gradient_checkpointing: true
    actor:
      strategy: fsdp
      use_kl_loss: true
      grad_clip: 1.0
      entropy_coeff: 0.01
      clip_ratio: 0.2
      clip_ratio_low: 0.2
      clip_ratio_high: 0.2
      policy_loss:
        ppo_kl_coef: 0.1
      kl_loss_coef: 0.1
      optim:
        lr: 3.0e-05
        weight_decay: 0.01
        lr_warmup_steps: 200
    rollout:
      strategy: fsdp
      dtype: bfloat16
    ref:
      strategy: fsdp
  critic:
    grad_clip: 1.0
    optim:
      lr: 3.0e-05
      weight_decay: 0.01
      lr_warmup_steps: 200
    model:
      enable_gradient_checkpointing: true
  algorithm:
    adv_estimator: grpo
    use_kl_in_reward: false
    kl_ctrl:
      kl_coef: 0.1
      target_kl: 0.15
