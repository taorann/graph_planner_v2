[DATA] train_files[0] JSONâ†’Parquet: datasets/r2e_gym/train.jsonl -> /workspace/graph_planner/rllm/rllm/data/datasets/graph_planner_repoenv/train_verl.parquet (rows=2)
[DATA] val_files[0] JSONâ†’Parquet: datasets/r2e_gym/val.jsonl -> /workspace/graph_planner/rllm/rllm/data/datasets/graph_planner_repoenv/val_verl.parquet (rows=1)
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9845.78it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1460.41it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 2 examples [00:00, 298.12 examples/s]
WARNING:2025-11-02 08:49:41,465:Overriding rollout tensor model parallel size to 1 for compatibility with FSDP
2025-11-02 08:49:46,859	WARNING utils.py:460 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-11-02 08:49:46,911	WARNING services.py:2155 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 1036664832 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.71gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
2025-11-02 08:49:47,205	INFO worker.py:2012 -- Started a local Ray instance.
/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(pid=gcs_server)[0m [2025-11-02 08:50:11,726 E 8842 8842] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[WARN] Failed to peek sample sandbox info: NotImplementedError('Loading a dataset cached in a LocalFileSystem is not supported.')
[33m(raylet)[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace="d5d22878-9ca8-4e42-9a87-bd4a9431f2b2", ...)
[INIT] Shared actors ready: planner_engine & cgm_tool
[36m(autoscaler +26s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[33m(autoscaler +26s)[0m Error: No available node types can fulfill resource request {'CPU': 1.0, 'GPU': 2.0}. Add suitable node types to this cluster to resolve this issue.
[33m(raylet)[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace="d5d22878-9ca8-4e42-9a87-bd4a9431f2b2", ...)
Traceback (most recent call last):
  File "/workspace/graph_planner/scripts/train_planner_grpo.py", line 551, in <module>
    main()
  File "/workspace/graph_planner/scripts/train_planner_grpo.py", line 544, in main
    trainer.train()
  File "/workspace/graph_planner/rllm/rllm/trainer/agent_trainer.py", line 80, in train
    ray.get(
  File "/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
                                  ^^^^^^^^^^^^^^^^^^^
  File "/root/.pyenv/versions/3.11.12/lib/python3.11/site-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ModuleNotFoundError): [36mray::TaskRunner.run()[39m (pid=9261, ip=172.30.0.194, actor_id=46443146ea800945afb85e5201000000, repr=<rllm.trainer.verl.train_agent_ppo.TaskRunner object at 0x7f045312d390>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/graph_planner/rllm/rllm/trainer/verl/train_agent_ppo.py", line 243, in run
    AgentPPOTrainer, _ = _import_agent_components()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/graph_planner/rllm/rllm/trainer/verl/train_agent_ppo.py", line 163, in _import_agent_components
    from rllm.trainer.verl.agent_ppo_trainer import AgentPPOTrainer  # local import to avoid optional deps at import time
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/graph_planner/rllm/rllm/trainer/verl/agent_ppo_trainer.py", line 19, in <module>
    from verl.trainer.ppo.ray_trainer import (
  File "/workspace/graph_planner/rllm/verl/verl/trainer/ppo/ray_trainer.py", line 43, in <module>
    from torchdata.stateful_dataloader import StatefulDataLoader
ModuleNotFoundError: No module named 'torchdata.stateful_dataloader'
[W1102 08:50:18.112998740 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
